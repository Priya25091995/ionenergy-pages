{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af3dbf70",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the API to send data\n",
    "factory_api = \"https://altergo.io/\"\n",
    "iot_api = \"https://iot.altergo.io/\"\n",
    "# Get API Key\n",
    "api_key = \"d365fd18cd5a14c1965349b6aa2f9253\"\n",
    "node_names = [\"SANTIAGO_6_N026\"]\n",
    "target_assets = [\"CA-ISO SANTIAGO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b742ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq git+https://bitbucket.org/freemens/ion_sdk@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e09c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ion_sdk.edison_api.models.factory import getSensorByName\n",
    "# from api.caiso import CAISO\n",
    "from ion_sdk.edison_api.edison_api import Client\n",
    "import collections\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e11ef510",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ALTERGO_FACTORY_API\"] = factory_api\n",
    "os.environ[\"ALTERGO_IOT_API\"] = iot_api\n",
    "os.environ[\"ALTERGO_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f4cff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAISO():\n",
    "    \"\"\"\n",
    "    Base Class containing useful methods to get data from OASIS provided by CAISO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node: str):\n",
    "        \"\"\"\n",
    "        Initialises the CAISO class with some helpful API calls.\n",
    "\n",
    "        Args:\n",
    "            node (str): PNODE_ID with CAISO.\n",
    "        \"\"\"\n",
    "\n",
    "        self.base_url = \"http://oasis.caiso.com/oasisapi/\"\n",
    "        # Check validity of node\n",
    "        if self.check_CAISO_node_validity(node):\n",
    "            self.node = node\n",
    "            self.AS_REGIONS = self.get_node_regions()\n",
    "        else:\n",
    "            raise ValueError(f\"PNODE_ID {node} is not a known node with the CAISO System.\")\n",
    "    \n",
    "    def check_CAISO_node_validity(self, node: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the node is a valid node with the CAISO.\n",
    "\n",
    "        Args:\n",
    "            node (str): PNODE_ID for the specific CAISO NODE.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if node is within the CAISO Node list. False if node is not present.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=ATL_AS_REGION_MAP\",\n",
    "                            f\"startdatetime={(datetime.now() - timedelta(days = 1)).strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={datetime.now().strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        return node in pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))[\"PNODE_ID\"].to_list()\n",
    "\n",
    "    def get_node_regions(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Gets a list of Ancillary Service regions in which the node participates in.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of Ancillary Service Regions that the node falls under\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\", \n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=ATL_AS_REGION_MAP\",\n",
    "                            f\"startdatetime={datetime.now().strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={(datetime.now()+timedelta(days=1)).strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"resultformat=6\",\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        return df[df[\"PNODE_ID\"] == self.node][\"AS_REGION_ID\"].to_list()\n",
    "\n",
    "    def get_DAM_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Day Ahead Market (DAM) Location Marginal Prices (LMP) for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for DAM at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_LMP\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"market_run_id=DAM\",\n",
    "                            f\"node={self.node}\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_to_return = df[df[\"LMP_TYPE\"] == \"LMP\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "        df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "        df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": \"DAM_LMP\"})\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_RTM_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Real Time Market (RTM) Location Marginal Prices (LMP) for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for RTM at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_INTVL_LMP\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"market_run_id=RTM\",\n",
    "                            f\"node={self.node}\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_to_return = df[df[\"LMP_TYPE\"] == \"LMP\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "        df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "        df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": \"RTM_LMP\"})\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_CD_RTM_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Contingency Dispatch Real Time Market (RTM) Location Marginal Prices (LMP) for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for Contingency Dispatch in thr RTM at the given node for the given time period.\n",
    "\n",
    "        Raises:\n",
    "            ValueError if there is no data between the start and end datetimes.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_CD_INTVL_LMP\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"market_run_id=RTM\",\n",
    "                            f\"node={self.node}\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        try:\n",
    "            df_to_return = df[df[\"LMP_TYPE\"] == \"LMP\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "            df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "            df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": \"CD_RTM_LMP\"})\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"There is no Contingency Dispatch Data between {start_datetime.strftime('%Y%m%dT%H:%M-0000')} and {end_datetime.strftime('%Y%m%dT%H:%M-0000')}\")\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_HASP_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Hour Ahead Scheduling Process (HASP) Location Marginal Prices (LMP) for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for the next 4 15-minute periods as per the HASP at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_HASP_LMP\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=3\",\n",
    "                            \"market_run_id=HASP\",\n",
    "                            f\"node={self.node}\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_to_return = df[df[\"LMP_TYPE\"] == \"LMP\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "        df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "        df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": \"HASP_LMP\"})\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_RTPD_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Real Time Pre-Dispatch (RTPD) Location Marginal Prices (LMP) for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for RTPD at the given node for the given time period. Each LMP is valid for a 15-minute period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_RTPD_LMP\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=3\",\n",
    "                            \"market_run_id=RTPD\",\n",
    "                            f\"node={self.node}\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_to_return = df[df[\"LMP_TYPE\"] == \"LMP\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"PRC\"]]\n",
    "        df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "        df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"PRC\": \"RTPD_LMP\"})\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_AS_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Ancillary Service (AS) Location Marginal Prices (LMP) for the various services for the node. Note that the start and end datetimes are in UTC. Note that the CAISO API returns the LMPs for one day prior to the start datetime (Midnight PST/PT reported at UTC)\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for AS at the given node in the DAM in the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_AS\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"market_run_id=DAM\",\n",
    "                            \"anc_type=ALL\",\n",
    "                            \"anc_region=ALL\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for anc_type in df[\"ANC_TYPE\"].unique():\n",
    "            series_to_sum = []\n",
    "            for anc_region in self.AS_REGIONS:\n",
    "                df_to_sum = df[(df[\"ANC_TYPE\"] == anc_type) & (df[\"ANC_REGION\"] == anc_region)].loc[:,[\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                df_to_sum[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_sum[\"INTERVALSTARTTIME_GMT\"])\n",
    "                df_to_sum = df_to_sum.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()[\"MW\"]\n",
    "                if not df_to_sum.empty:\n",
    "                    series_to_sum.append(df_to_sum)\n",
    "            if len(series_to_sum) > 0:\n",
    "                df_list.append(pd.DataFrame(sum(series_to_sum)).rename(columns= {\"MW\":f\"{anc_type}_MW\"}))\n",
    "\n",
    "        df_to_return = pd.concat(df_list, axis = 1)\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_AS_RTM_LMP(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Ancillary Service (AS) Real Time Market (RTM) Location Marginal Prices (LMP) for the various services for the node. Note that the start and end datetimes are in UTC. Further note that this method only yield the immediate 4 LMPs after the start datetime.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for AS at the given node in the RTM.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=PRC_INTVL_AS\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"market_run_id=RTM\",\n",
    "                            \"anc_type=ALL\",\n",
    "                            \"anc_region=ALL\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for anc_type in df[\"ANC_TYPE\"].unique():\n",
    "            series_to_sum = []\n",
    "            for anc_region in self.AS_REGIONS:\n",
    "                df_to_sum = df[(df[\"ANC_TYPE\"] == anc_type) & (df[\"ANC_REGION\"] == anc_region)].loc[:,[\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                df_to_sum[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_sum[\"INTERVALSTARTTIME_GMT\"])\n",
    "                df_to_sum = df_to_sum.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()[\"MW\"]\n",
    "                if not df_to_sum.empty:\n",
    "                    series_to_sum.append(df_to_sum)\n",
    "            if len(series_to_sum) > 0:\n",
    "                df_list.append(pd.DataFrame(sum(series_to_sum)).rename(columns= {\"MW\":f\"{anc_type}_MW\"}))\n",
    "\n",
    "        df_to_return = pd.concat(df_list, axis = 1)\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_SLD_FCST(self, start_datetime: datetime, end_datetime: datetime, market: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Demand Forecast for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "            market (str): ID for market demand forecast. Allowed values are \"DAM\", \"2DA\", \"7DA\", \"RTD\".\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the Demand Forecast for the given market type at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        warnings.warn(\"Demand Forecast can only find demand forecast for the CA ISO-TAC area.\")\n",
    "\n",
    "        if market in [\"DAM\", \"2DA\", \"7DA\"]:\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=SLD_FCST\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        elif market == \"RTM\":\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=SLD_FCST\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"resultformat=6\",\n",
    "                                \"execution_type=RTD\"\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input for market {market}. It should be one of 'DAM', '2DA, '7DA', or 'RTD'.\")\n",
    "\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        if zip_file.namelist()[0][-3:] == \"csv\":\n",
    "            df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "            df_to_return = df[df[\"TAC_AREA_NAME\"] == \"CA ISO-TAC\"].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "            df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "            df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": f\"{market}_MW\"})\n",
    "            return df_to_return.sort_index()\n",
    "        else:\n",
    "            raise ValueError(f\"There is no valid file between {start_datetime.strftime('%Y-%m-%d %H:%M:00')} and {end_datetime.strftime('%Y-%m-%d %H:%M:00')}.\")\n",
    "\n",
    "    def get_SLD_REN_FCST(self, start_datetime: datetime, end_datetime: datetime, market: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Renewable Energy Demand Forecast for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "            market (str): ID for market demand forecast. Allowed values are \"DAM\", \"RTPD\", \"RTD\".\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the System Renewable Energy Forecast for the given market type at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        if market in [\"DAM\", \"RTPD\", \"RTD\"]:\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=SLD_REN_FCST\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input for market {market}. It should be one of 'DAM', 'RTPD, or 'RTD'.\")\n",
    "\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for renew_type in df[\"RENEWABLE_TYPE\"].unique():\n",
    "            for trading_hub in df[\"TRADING_HUB\"].unique():\n",
    "                df_to_return = df[(df[\"TRADING_HUB\"] == trading_hub) & (df[\"RENEWABLE_TYPE\"] == renew_type)].loc[:, [\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                df_to_return[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_return[\"INTERVALSTARTTIME_GMT\"])\n",
    "                df_to_return = df_to_return.set_index(\"INTERVALSTARTTIME_GMT\").rename(columns = {\"MW\": f\"{renew_type}_{trading_hub}_MW\"})\n",
    "                if not df_to_return.empty:\n",
    "                    df_list.append(df_to_return)\n",
    "        df_to_return = df_list[0]\n",
    "        if len(df_list) > 1:\n",
    "            for x in df_list[1:]:\n",
    "                df_to_return = df_to_return.join(x, how=\"outer\")\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_AS_REQ(self, start_datetime: datetime, end_datetime: datetime, market: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Ancillary Service (AS) Requirement for the various services for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "            market (str): ID for market requrement. Allowed values are \"DAM\", or \"RTM\".\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for RTM at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        if market in [\"DAM\", \"RTM\"]:\n",
    "\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=AS_REQ\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"anc_type=ALL\",\n",
    "                                \"anc_region=ALL\",\n",
    "                                \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input for market {market}. It should be one of 'DAM', or 'RTM'.\")\n",
    "\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for anc_type in df[\"ANC_TYPE\"].unique():\n",
    "            for xml_data in df[\"XML_DATA_ITEM\"].unique():\n",
    "                series_to_sum = []\n",
    "                for anc_region in self.AS_REGIONS:\n",
    "                    df_to_sum = df[(df[\"ANC_TYPE\"] == anc_type) & (df[\"ANC_REGION\"] == anc_region) & (df[\"XML_DATA_ITEM\"] == xml_data)].loc[:,[\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                    df_to_sum[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_sum[\"INTERVALSTARTTIME_GMT\"])\n",
    "                    df_to_sum = df_to_sum.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()[\"MW\"]\n",
    "                    if not df_to_sum.empty:\n",
    "                        series_to_sum.append(df_to_sum)\n",
    "                if len(series_to_sum) > 0:\n",
    "                    df_list.append(pd.DataFrame(sum(series_to_sum)).rename(columns= {\"MW\":f\"{xml_data}\"}))\n",
    "\n",
    "        df_to_return = pd.concat(df_list, axis = 1)\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_AS_RESULTS(self, start_datetime: datetime, end_datetime: datetime, market: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Ancillary Service (AS) Results for the various services for the node. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "            market (str): ID for market requrement. Allowed values are \"DAM\", or \"RTM\".\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for RTM at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        if market in [\"DAM\", \"RTM\"]:\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=AS_RESULTS\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"anc_type=ALL\",\n",
    "                                \"anc_region=ALL\",\n",
    "                                \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input for market {market}. It should be one of 'DAM', or 'RTM'.\")\n",
    "\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for xml_data in df[\"XML_DATA_ITEM\"].unique():\n",
    "            series_to_sum = []\n",
    "            for anc_region in self.AS_REGIONS:\n",
    "                df_to_sum = df[(df[\"ANC_REGION\"] == anc_region) & (df[\"XML_DATA_ITEM\"] == xml_data)].loc[:,[\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                df_to_sum[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_sum[\"INTERVALSTARTTIME_GMT\"])\n",
    "                df_to_sum = df_to_sum.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()[\"MW\"]\n",
    "                if not df_to_sum.empty:\n",
    "                    series_to_sum.append(df_to_sum)\n",
    "            if len(series_to_sum) > 0:\n",
    "                df_list.append(pd.DataFrame(sum(series_to_sum)).rename(columns= {\"MW\":f\"{xml_data}\"}))\n",
    "\n",
    "        df_to_return = pd.concat(df_list, axis=1)\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_AS_MILEAGE_CALC(self, start_datetime: datetime, end_datetime: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the Ancillary Service (AS) Mileage Calculations for the various services with CAISO. Note that the start and end datetimes are in UTC.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "\n",
    "        Returns\n",
    "            pd.DataFrame: A DataFrame which contains the LMP for RTM at the given node for the given time period.\n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(\n",
    "            \"?\".join(\n",
    "                [\n",
    "                    f\"{self.base_url}SingleZip\",\n",
    "                    \"&\".join(\n",
    "                        [\n",
    "                            \"queryname=AS_MILEAGE_CALC\",\n",
    "                            f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                            \"version=1\",\n",
    "                            \"anc_type=ALL\",\n",
    "                            \"resultformat=6\",\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df_list = []\n",
    "        for anc_type in df[\"ANC_TYPE\"].unique():\n",
    "            for xml_data_type in df[\"XML_DATA_TYPE\"].unique():\n",
    "                df_to_sum = df[(df[\"ANC_TYPE\"] == anc_type) & (df[\"ANC_REGION\"] == \"AS_CAISO_EXP\") & (df[\"XML_DATA_TYPE\"] == xml_data_type)].loc[:,[\"INTERVALSTARTTIME_GMT\", \"MW\"]]\n",
    "                df_to_sum[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df_to_sum[\"INTERVALSTARTTIME_GMT\"])\n",
    "                df_to_sum = df_to_sum.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()[\"MW\"]\n",
    "                if not df_to_sum.empty:\n",
    "                    df_list.append(pd.DataFrame(df_to_sum).rename(columns= {\"MW\":f\"{xml_data_type}\"}))\n",
    "\n",
    "        df_to_return = pd.concat(df_list, axis = 1)\n",
    "\n",
    "        return df_to_return.sort_index()\n",
    "\n",
    "    def get_ENE_SLRS(self, start_datetime: datetime, end_datetime: datetime, market: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches the balanced load schedule for the specific market within the given time range.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): Start date for the prices in UTC.\n",
    "            end_date (datetime): End date for the prices in UTC.\n",
    "            market (str): ID for market requrement. Allowed values are \"DAM\", \"HASP\", \"RUC\", or \"RTM\".\n",
    "        \"\"\"\n",
    "\n",
    "        if market in [\"DAM\", \"HASP\", \"RUC\", \"RTM\"]:\n",
    "            response = requests.get(\n",
    "                \"?\".join(\n",
    "                    [\n",
    "                        f\"{self.base_url}SingleZip\",\n",
    "                        \"&\".join(\n",
    "                            [\n",
    "                                \"queryname=ENE_SLRS\",\n",
    "                                f\"startdatetime={start_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                f\"enddatetime={end_datetime.strftime('%Y%m%dT%H:%M-0000')}\",\n",
    "                                \"version=1\",\n",
    "                                f\"market_run_id={market}\",\n",
    "                                \"tac_zone_name=ALL\",\n",
    "                                \"schedule=ALL\",\n",
    "                                \"resultformat=6\",\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input for market {market}. It should be one of 'DAM', 'HASP', 'RUC', or 'RTM'.\")\n",
    "        \n",
    "        zip_file = ZipFile(BytesIO(response.content))\n",
    "        df = pd.read_csv(BytesIO(zip_file.read(zip_file.namelist()[0])))\n",
    "        df[\"INTERVALSTARTTIME_GMT\"] = pd.to_datetime(df[\"INTERVALSTARTTIME_GMT\"])\n",
    "        df = df.set_index(\"INTERVALSTARTTIME_GMT\").sort_index()\n",
    "        if market == \"DAM\":\n",
    "            df_to_return = df[(df[\"TAC_ZONE_NAME\"] == \"Caiso_Totals\") & (df[\"SCHEDULE\"] == \"Load\")].loc[:, [\"MW\"]]\n",
    "        else:\n",
    "            df = df[(df[\"TAC_ZONE_NAME\"] == \"Caiso_Totals\") & (df[\"SCHEDULE\"] != \"Load\")].loc[:, [\"MW\", \"SCHEDULE\"]]\n",
    "            df_to_return = -1 * df[df[\"SCHEDULE\"] == \"Import\"]\n",
    "            for load in df[\"SCHEDULE\"].unique():\n",
    "                if load == \"Export\" or load == \"Generation\":\n",
    "                    df_to_return += df[df[\"SCHEDULE\"] == load]\n",
    "                else:\n",
    "                    pass\n",
    "            df_to_return = df_to_return.drop(columns=[\"SCHEDULE\"])\n",
    "        df_to_return = df_to_return.rename(columns = {\"MW\":f\"{market}_MW\"})\n",
    "        return df_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b12e635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configurations(config_fp: str) -> Tuple[list, list, str]:\n",
    "    \"\"\"\n",
    "    A function to read the configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_fp (str): Filepath to the configurations files.\n",
    "\n",
    "    Returns:\n",
    "        serial_numbers [list]: List of serial numbers read from the configurations file.\n",
    "        nodes [list]: List of nodes read from the configuration file.\n",
    "        blueprint_name: Blueprint name for the CAISO assets on the platform.\n",
    "    \"\"\"\n",
    "#     config = json.load(open(config_fp, \"r\"))\n",
    "#     os.environ[\"ALTERGO_FACTORY_API\"] = config[\"ALTERGO_FACTORY_API\"]\n",
    "#     os.environ[\"ALTERGO_IOT_API\"] = config[\"ALTERGO_IOT_API\"]\n",
    "#     os.environ[\"ALTERGO_API_KEY\"] = config[\"ALTERGO_API_KEY\"]\n",
    "    serial_numbers = [x for x in target_assets]\n",
    "    nodes = [x for x in node_names]\n",
    "#     blueprint_name = config[\"blueprint_name\"]\n",
    "    return serial_numbers, nodes\n",
    "\n",
    "# Create a client to connect to the platform\n",
    "def create_session(serial_numbers: list[str], blueprint_name: str = \"California Independent System Operator (CAISO) Node\"):\n",
    "    if os.environ.get(\"ALTERGO_API_KEY\") is not None:\n",
    "        edApi = Client(os.environ.get(\"ALTERGO_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find API Key in the environment.\")\n",
    "\n",
    "    assets = []\n",
    "    for sn in serial_numbers:\n",
    "        if edApi.getAsset(sn) is None:\n",
    "            edApi.createAssetBySerial(blueprint_name, sn)\n",
    "        assets.append(edApi.getAsset(sn))\n",
    "    assert not any([True for x in assets if x is None]), \"A serial number could not be found on the platform.\"\n",
    "    \n",
    "    return edApi, assets\n",
    "\n",
    "def run():\n",
    "    serial_numbers, nodes = read_configurations(\"config.json\")\n",
    "    edApi, assets = create_session(serial_numbers)\n",
    "\n",
    "    for i, x in enumerate(nodes):\n",
    "        nodes[i] = CAISO(x)\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for node, asset in zip(nodes, assets):\n",
    "            # Loops to run once an hour\n",
    "            start_datetime = datetime.now() - timedelta(days = 1)\n",
    "            end_datetime = datetime.now()\n",
    "            try:\n",
    "                # Get DAM LMP\n",
    "                df = node.get_DAM_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"DAM_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get HASP LMP\n",
    "                df = node.get_HASP_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"HASP_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get RTPD LMP\n",
    "                df = node.get_RTPD_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"RTPD_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "            \n",
    "            try:\n",
    "                # Get RTM LMP\n",
    "                df = node.get_RTM_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"RTM_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get DAM AS\n",
    "                df = node.get_AS_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                array_map = json.loads(getSensorByName(asset.model, \"AS_LMP\").format)\n",
    "                df = df[array_map]\n",
    "                df.loc[:, \"AS_LMP\"] = [[x for x in row] for _, row in df.iterrows()]\n",
    "                df = df.drop(columns = array_map)\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"AS_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get RTM AS\n",
    "                df = node.get_AS_RTM_LMP(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                array_map = json.loads(getSensorByName(asset.model, \"AS_RTM_LMP\").format)\n",
    "                df = df[array_map]\n",
    "                df.loc[:, \"AS_RTM_LMP\"] = [[x for x in row] for _, row in df.iterrows()]\n",
    "                df = df.drop(columns = array_map)\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"AS_RTM_LMP gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get SLD DAM FCST\n",
    "                df_list = []\n",
    "                start_datetime = datetime.now()\n",
    "                end_datetime = datetime.now() + timedelta(days = 1)\n",
    "                df = node.get_SLD_FCST(start_datetime, end_datetime, \"DAM\")\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_FCST in DAM gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get SLD 2DA FCST\n",
    "                start_datetime = datetime.now()\n",
    "                end_datetime = datetime.now() + timedelta(days = 2)\n",
    "                df = node.get_SLD_FCST(start_datetime, end_datetime, \"2DA\")\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_FCST in 2DA gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get SLD 7DA FCST\n",
    "                start_datetime = datetime.now()\n",
    "                end_datetime = datetime.now() + timedelta(days = 7)\n",
    "                df = node.get_SLD_FCST(start_datetime, end_datetime, \"7DA\")\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_FCST in 7DA gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get SLD RTM FCST\n",
    "                start_datetime = datetime.now() - timedelta(days = 1)\n",
    "                end_datetime = datetime.now()\n",
    "                df = node.get_SLD_FCST(start_datetime, end_datetime, \"RTM\")\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_FCST in RTM gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                df = pd.concat(df_list,axis=1)\n",
    "                fcst_cols = json.loads(getSensorByName(asset.model, \"SLD_FCST\").format)\n",
    "                df.loc[:, \"SLD_FCST\"] = [[x for x in row] for _, row in df[fcst_cols].iterrows()]\n",
    "                df = df.drop(columns=fcst_cols)\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_FCST  while merging gave an exception.\")\n",
    "\n",
    "            try:\n",
    "                # Get SLD REN FCST\n",
    "                start_datetime = datetime.now()\n",
    "                end_datetime = datetime.now() + timedelta(days = 1)\n",
    "                df = node.get_SLD_REN_FCST(start_datetime, end_datetime, \"DAM\")\n",
    "                ren_fcst_cols = json.loads(getSensorByName(asset.model, \"DAM_REN_FCST\").format)\n",
    "                if \"Wind_ZP26_MW\" not in df.columns:\n",
    "                    df[\"Wind_ZP26_MW\"] = np.NaN\n",
    "                df.loc[:, \"DAM_REN_FCST\"] = [[x for x in row] for _, row in df[ren_fcst_cols].iterrows()]\n",
    "                df = df.drop(columns = ren_fcst_cols)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_REN_FCST in DAM gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            \n",
    "            try:\n",
    "                start_datetime = datetime.now() - timedelta(days = 1)\n",
    "                end_datetime = datetime.now()\n",
    "                df = node.get_SLD_REN_FCST(start_datetime, end_datetime, \"RTPD\")\n",
    "                ren_fcst_cols = json.loads(getSensorByName(asset.model, \"RTPD_REN_FCST\").format)\n",
    "                if \"Wind_ZP26_MW\" not in df.columns:\n",
    "                    df[\"Wind_ZP26_MW\"] = np.NaN\n",
    "                df.loc[:, \"RTPD_REN_FCST\"] = [[x for x in row] for _, row in df[ren_fcst_cols].iterrows()]\n",
    "                df = df.drop(columns = ren_fcst_cols)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLF_REN_FCST in RTPD gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                df = node.get_SLD_REN_FCST(start_datetime, end_datetime, \"RTD\")\n",
    "                ren_fcst_cols = json.loads(getSensorByName(asset.model, \"RTD_REN_FCST\").format)\n",
    "                if \"Wind_ZP26_MW\" not in df.columns:\n",
    "                    df[\"Wind_ZP26_MW\"] = np.NaN\n",
    "                df.loc[:, \"RTD_REN_FCST\"] = [[x for x in row] for _, row in df[ren_fcst_cols].iterrows()]\n",
    "                df = df.drop(columns = ren_fcst_cols)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"SLD_REN_FCST in RTD gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get DAM AS REQ\n",
    "                start_datetime = datetime.now()\n",
    "                end_datetime = datetime.now() + timedelta(days = 1)\n",
    "                df = node.get_AS_REQ(start_datetime, end_datetime, \"DAM\")\n",
    "                req_cols = json.loads(getSensorByName(asset.model, \"DAM_AS_REQ\").format)\n",
    "                df = df[req_cols]\n",
    "                df.loc[:,\"DAM_AS_REQ\"] = [[x for x in row] for _, row in df.iterrows()]\n",
    "                df = df.drop(columns=req_cols)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"AS_REQ in DAM gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get RTM AS REQ\n",
    "                start_datetime = datetime.now() - timedelta(days = 1)\n",
    "                end_datetime = datetime.now()\n",
    "                df = node.get_AS_REQ(start_datetime, end_datetime, \"RTM\")\n",
    "                req_cols = json.loads(getSensorByName(asset.model, \"RTM_AS_REQ\").format)\n",
    "                df = df[req_cols]\n",
    "                df.loc[:, \"RTM_AS_REQ\"] = [[x for x in row] for _, row in df.iterrows()]\n",
    "                df = df.drop(columns=req_cols)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"AS_REQ in RTM gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                # Get AS MILEAGE CALC\n",
    "                start_datetime = datetime.now() - timedelta(days = 1)\n",
    "                end_datetime = datetime.now()\n",
    "                df = node.get_AS_MILEAGE_CALC(start_datetime, end_datetime)\n",
    "                # df.index = df.index.tz_convert(\"America/Los_Angeles\")\n",
    "                as_mileage_cols = json.loads(getSensorByName(asset.model, \"AS_MILEAGE_CALC\").format)\n",
    "                df.loc[:, \"AS_MILEAGE_CALC\"] = [[x for x in row] for _, row in df[as_mileage_cols].iterrows()]\n",
    "                df = df.drop(columns=as_mileage_cols)\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"AS_MILEAGE_CALC gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "            try:\n",
    "                df_list = []\n",
    "                for market in [\"DAM\", \"HASP\", \"RUC\", \"RTM\"]:\n",
    "                    df_list.append(node.get_ENE_SLRS(start_datetime, end_datetime, market))\n",
    "                ene_cols = json.loads(getSensorByName(asset.model, \"ENE_SLRS\").format)\n",
    "                df = pd.concat(df_list, axis=1)[ene_cols]\n",
    "                df.loc[:, \"ENE_SLRS\"] = [[x for x in row] for _, row in df.iterrows()]\n",
    "                df.index = df.index.tz_convert(tz=None)\n",
    "                df = df.drop(columns = ene_cols)\n",
    "                asset.df = df\n",
    "                edApi.updateSensorDataByDirectInsert(asset, asset.df.columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"ENE_SLRS gave an exception for the time range {start_datetime.strftime('%y-%m-%d %H:%M:%S')} to {end_datetime.strftime('%y-%m-%d %H:%M:%S')}.\")\n",
    "\n",
    "\n",
    "        time.sleep(300 - (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3aad8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n",
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n",
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n",
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n",
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n",
      "\n",
      "Preparing payload\n",
      "Payload sent successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2527/3781640346.py:382: UserWarning: Demand Forecast can only find demand forecast for the CA ISO-TAC area.\n",
      "  warnings.warn(\"Demand Forecast can only find demand forecast for the CA ISO-TAC area.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2527/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2527/1449508038.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mstart_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mend_datetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_SLD_FCST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_datetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_datetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# df.index = df.index.tz_convert(\"America/Los_Angeles\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2527/3781640346.py\u001b[0m in \u001b[0;36mget_SLD_FCST\u001b[0;34m(self, start_datetime, end_datetime, market)\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmarket\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RTM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             response = requests.get(\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \"?\".join(\n\u001b[1;32m    405\u001b[0m                     [\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m                 decoded = self._decode(\n\u001b[1;32m    769\u001b[0m                     \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0mreturned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# amt > self.chunk_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0mreturned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Toss the CRLF at the end of the chunk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/development/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/development/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a637a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
